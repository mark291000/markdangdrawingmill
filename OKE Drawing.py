import streamlit as st
import pdfplumber
import re
import pandas as pd
from collections import defaultdict
import os
import tempfile
import io
import math
import time
import traceback
from contextlib import contextmanager
import gc

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title="PDF Data Extractor",
    page_icon="üìÑ",
    layout="wide"
)

# --- CONTEXT MANAGER ƒê·ªÇ X·ª¨ L√ù FILE AN TO√ÄN ---
@contextmanager
def safe_pdf_processing(pdf_path, timeout=30):
    """Context manager ƒë·ªÉ x·ª≠ l√Ω PDF an to√†n v·ªõi timeout"""
    start_time = time.time()
    pdf_obj = None
    try:
        pdf_obj = pdfplumber.open(pdf_path)
        yield pdf_obj
    except Exception as e:
        st.error(f"L·ªói khi m·ªü PDF: {str(e)}")
        raise e
    finally:
        # ƒê·∫£m b·∫£o ƒë√≥ng file
        if pdf_obj:
            try:
                pdf_obj.close()
            except:
                pass
        # Ki·ªÉm tra timeout
        if time.time() - start_time > timeout:
            st.warning("‚ö†Ô∏è File x·ª≠ l√Ω qu√° l√¢u, c√≥ th·ªÉ b·ªã timeout")
        # D·ªçn d·∫πp memory
        gc.collect()

# --- H√ÄM X·ª¨ L√ù FILE TEMP AN TO√ÄN ---
def save_uploaded_file_safe(uploaded_file):
    """L∆∞u file upload m·ªôt c√°ch an to√†n"""
    temp_path = None
    try:
        # T·∫°o file t·∫°m th·ªùi
        temp_fd, temp_path = tempfile.mkstemp(suffix='.pdf', prefix='pdf_extract_')
        
        # Ghi d·ªØ li·ªáu v√†o file
        with os.fdopen(temp_fd, 'wb') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file.flush()
            os.fsync(tmp_file.fileno())  # ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c ghi v√†o disk
        
        return temp_path
    except Exception as e:
        st.error(f"L·ªói khi l∆∞u file {uploaded_file.name}: {str(e)}")
        # D·ªçn d·∫πp n·∫øu c√≥ l·ªói
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except:
                pass
        return None

def cleanup_temp_file(file_path):
    """D·ªçn d·∫πp file t·∫°m th·ªùi m·ªôt c√°ch an to√†n"""
    if file_path and os.path.exists(file_path):
        try:
            os.unlink(file_path)
            return True
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ x√≥a file t·∫°m: {str(e)}")
            return False
    return True

# --- C√ÅC H√ÄM PH·ª§ TR·ª¢ (LOGIC X·ª¨ L√ù) ---
def find_dimension_lines(lines, tolerance=2):
    """T√¨m c√°c ƒë∆∞·ªùng dimension v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        horizontal_lines = [line for line in lines if abs(line['y0'] - line['y1']) <= tolerance]
        vertical_lines = [line for line in lines if abs(line['x0'] - line['x1']) <= tolerance]
        return horizontal_lines, vertical_lines
    except Exception:
        return [], []

def is_near_dimension_line(number_bbox, h_lines, v_lines, tolerance=15):
    """Ki·ªÉm tra c√≥ g·∫ßn ƒë∆∞·ªùng dimension kh√¥ng v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        num_x_center = (number_bbox['x0'] + number_bbox['x1']) / 2
        num_y_center = (number_bbox['top'] + number_bbox['bottom']) / 2
        
        for h_line in h_lines:
            line_x_min, line_x_max = min(h_line['x0'], h_line['x1']), max(h_line['x0'], h_line['x1'])
            if (abs(h_line['top'] - num_y_center) < tolerance) and (line_x_min < num_x_center < line_x_max):
                left_tick = any(abs(v['x0'] - line_x_min) < 2 for v in v_lines)
                right_tick = any(abs(v['x0'] - line_x_max) < 2 for v in v_lines)
                if left_tick and right_tick: return True
                
        for v_line in v_lines:
            line_y_min, line_y_max = min(v_line['top'], v_line['bottom']), max(v_line['top'], v_line['bottom'])
            if (abs(v_line['x0'] - num_x_center) < tolerance) and (line_y_min < num_y_center < line_y_max):
                top_tick = any(abs(h['top'] - line_y_min) < 2 for h in h_lines)
                bottom_tick = any(abs(h['top'] - line_y_max) < 2 for h in h_lines)
                if top_tick and bottom_tick: return True
    except Exception:
        pass
    return False

def is_bbox_inside_zones(bbox, zones):
    """Ki·ªÉm tra bbox c√≥ trong zones kh√¥ng"""
    try:
        for zone in zones:
            if (max(bbox['x0'], zone['x0']) < min(bbox['x1'], zone['x1']) and 
                max(bbox['top'], zone['top']) < min(bbox['bottom'], zone['bottom'])):
                return True
    except Exception:
        pass
    return False

def get_ink_area_of_first_char(cluster, page):
    """T√≠nh ink area v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        if not cluster: return 0.0
        first_char = cluster[0]
        char_bbox = (first_char['x0'], first_char['top'], first_char['x1'], first_char['bottom'])
        cropped_char_page = page.crop(char_bbox)
        ink_area = 0.0
        
        for rect in cropped_char_page.rects: 
            ink_area += rect['width'] * rect['height']
        for line in cropped_char_page.lines: 
            ink_area += math.sqrt((line['x1'] - line['x0'])**2 + (line['y1'] - line['y0'])**2) * line.get('linewidth', 1)
        return ink_area
    except Exception:
        return 0.0

def calculate_confidence(number_info):
    """T√≠nh confidence score"""
    try:
        score = 20
        if number_info['is_near_dimension_line']: score += 50
        if number_info['ink_area'] > 15: score += 25
        elif number_info['ink_area'] > 8: score += 15
        if number_info['bbox']['top'] > number_info['page_height'] * 0.85: score -= 40
        if number_info['orientation'] == 'Horizontal': score += 5
        else: score -= 5
        return max(0, min(100, score))
    except Exception:
        return 0

def process_cluster_for_new_logic(cluster, page, orientation, h_lines, v_lines, date_zones):
    """X·ª≠ l√Ω cluster v·ªõi x·ª≠ l√Ω l·ªói to√†n di·ªán"""
    try:
        if not cluster: return None
        number_str = "".join([c['text'] for c in cluster])
        if orientation == 'Vertical': number_str = number_str[::-1]
        
        if number_str.isdigit() and int(number_str) < 3500:
            value = int(number_str)
            bbox = {
                'x0': min(c['x0'] for c in cluster), 
                'top': min(c['top'] for c in cluster), 
                'x1': max(c['x1'] for c in cluster), 
                'bottom': max(c['bottom'] for c in cluster)
            }
            
            if is_bbox_inside_zones(bbox, date_zones): return None
            ink_area = get_ink_area_of_first_char(cluster, page)
            if ink_area > 200: return None
            
            is_dim_line = is_near_dimension_line(bbox, h_lines, v_lines)
            number_info = {
                'value': value, 'bbox': bbox, 'ink_area': ink_area, 
                'orientation': orientation, 'is_near_dimension_line': is_dim_line, 
                'page_height': page.height
            }
            confidence = calculate_confidence(number_info)
            return {'Number': value, 'Ink Area': round(ink_area, 2), 'Confidence (%)': confidence}
    except Exception as e:
        # Log l·ªói nh∆∞ng kh√¥ng crash
        pass
    return None

def extract_all_numbers_safe(pdf_path):
    """Tr√≠ch xu·∫•t s·ªë v·ªõi x·ª≠ l√Ω l·ªói v√† timeout"""
    all_numbers_data = []
    
    try:
        with safe_pdf_processing(pdf_path, timeout=60) as pdf:
            for page_num, page in enumerate(pdf.pages):
                try:
                    # Ki·ªÉm tra timeout cho t·ª´ng page
                    start_page_time = time.time()
                    
                    lines = page.lines if hasattr(page, 'lines') else []
                    h_lines, v_lines = find_dimension_lines(lines)
                    
                    # T√¨m date zones v·ªõi x·ª≠ l√Ω l·ªói
                    try:
                        date_zones = page.search(r'\b\d{1,2}/\d{1,2}/\d{4}\b', regex=True) or []
                    except:
                        date_zones = []
                    
                    # X·ª≠ l√Ω horizontal chars
                    try:
                        h_chars = sorted([c for c in page.chars if c.get("upright", True)], 
                                       key=lambda c: (round(c["top"], 1), c["x0"]))
                        current_cluster, last_char = [], None
                        
                        for char in h_chars:
                            # Ki·ªÉm tra timeout
                            if time.time() - start_page_time > 30:
                                st.warning(f"‚ö†Ô∏è Page {page_num + 1} x·ª≠ l√Ω qu√° l√¢u, b·ªè qua")
                                break
                                
                            if char['text'].isdigit():
                                if (current_cluster and last_char and 
                                    (char['x0'] - last_char['x1'] > char.get('size', 8) * 0.6 or 
                                     abs(char['top'] - last_char['top']) > 2)):
                                    result = process_cluster_for_new_logic(current_cluster, page, 'Horizontal', h_lines, v_lines, date_zones)
                                    if result: all_numbers_data.append(result)
                                    current_cluster = [char]
                                else: 
                                    current_cluster.append(char)
                            else:
                                if current_cluster:
                                    result = process_cluster_for_new_logic(current_cluster, page, 'Horizontal', h_lines, v_lines, date_zones)
                                    if result: all_numbers_data.append(result)
                                current_cluster = []
                            last_char = char
                        
                        if current_cluster:
                            result = process_cluster_for_new_logic(current_cluster, page, 'Horizontal', h_lines, v_lines, date_zones)
                            if result: all_numbers_data.append(result)
                    except Exception as e:
                        st.warning(f"L·ªói x·ª≠ l√Ω horizontal chars ·ªü page {page_num + 1}: {str(e)}")
                    
                    # X·ª≠ l√Ω vertical chars
                    try:
                        v_chars_by_col = defaultdict(list)
                        for char in [c for c in page.chars if not c.get("upright", True)]: 
                            v_chars_by_col[round(char['x0'], 0)].append(char)
                        
                        for col in v_chars_by_col.values():
                            if time.time() - start_page_time > 30:
                                break
                                
                            col.sort(key=lambda c: c['top'])
                            current_cluster, last_char = [], None
                            
                            for char in col:
                                if char['text'].isdigit():
                                    if (current_cluster and last_char and 
                                        (char['top'] - last_char['bottom'] > char.get('size', 8) * 0.6)):
                                        result = process_cluster_for_new_logic(current_cluster, page, 'Vertical', h_lines, v_lines, date_zones)
                                        if result: all_numbers_data.append(result)
                                        current_cluster = [char]
                                    else: 
                                        current_cluster.append(char)
                                else:
                                    if current_cluster:
                                        result = process_cluster_for_new_logic(current_cluster, page, 'Vertical', h_lines, v_lines, date_zones)
                                        if result: all_numbers_data.append(result)
                                    current_cluster = []
                                last_char = char
                            
                            if current_cluster:
                                result = process_cluster_for_new_logic(current_cluster, page, 'Vertical', h_lines, v_lines, date_zones)
                                if result: all_numbers_data.append(result)
                    except Exception as e:
                        st.warning(f"L·ªói x·ª≠ l√Ω vertical chars ·ªü page {page_num + 1}: {str(e)}")
                        
                except Exception as e:
                    st.warning(f"L·ªói x·ª≠ l√Ω page {page_num + 1}: {str(e)}")
                    continue
                    
    except Exception as e:
        st.error(f"L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω PDF: {str(e)}")
        
    return all_numbers_data

def assign_ink_groups(df, tolerance=1.0):
    """G√°n ink groups v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        if df.empty or 'Ink Area' not in df.columns:
            df['Ink Area Group'] = 0
            return df
        unique_inks = sorted(df['Ink Area'].unique())
        if not unique_inks:
            df['Ink Area Group'] = 0
            return df
        group_mapping, current_group_id = {}, 1
        group_mapping[unique_inks[0]] = current_group_id
        last_val_in_group = unique_inks[0]
        for ink in unique_inks[1:]:
            if ink - last_val_in_group <= tolerance:
                group_mapping[ink] = current_group_id
            else:
                current_group_id += 1
                group_mapping[ink] = current_group_id
            last_val_in_group = ink
        df['Ink Area Group'] = df['Ink Area'].map(group_mapping)
        return df
    except Exception:
        df['Ink Area Group'] = 0
        return df

def find_laminate_keywords_safe(pdf_path):
    """T√¨m laminate keywords v·ªõi x·ª≠ l√Ω l·ªói"""
    target_keywords = ["LAM/MASKING (IF APPLICABLE)","GLUEABLE LAM/TC BLACK (IF APPLICABLE)",
                      "FLEX PAPER/PAPER", "GLUEABLE LAM", "RAW", "LAM", "GRAIN"]
    found_pairs = []
    
    try:
        with safe_pdf_processing(pdf_path, timeout=30) as pdf:
            for page in pdf.pages:
                try:
                    chars = page.chars
                    if not chars: continue
                    chars = sorted(chars, key=lambda c: (round(c["top"], 1), c["x0"]))
                    full_text = "".join(c["text"] for c in chars)
                    
                    for keyword in target_keywords:
                        try:
                            for pos in [m.start() for m in re.finditer(re.escape(keyword), full_text)]:
                                keyword_chars = chars[pos:pos + len(keyword)]
                                if not keyword_chars: continue
                                keyword_y = sum(c["top"] for c in keyword_chars) / len(keyword_chars)
                                keyword_x_center = sum(c["x0"] for c in keyword_chars) / len(keyword_chars)
                                below_word = find_word_below_safe(chars, keyword_y, keyword_x_center, target_keywords)
                                if below_word: found_pairs.append(f"{keyword}/{below_word}")
                                else: found_pairs.append(keyword)
                        except Exception:
                            continue
                except Exception:
                    continue
    except Exception:
        pass
        
    return found_pairs

def find_word_below_safe(chars, keyword_y, keyword_x_center, target_keywords, y_tolerance=50, x_tolerance=100):
    """T√¨m word below v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        chars_below = [c for c in chars if c["top"] > keyword_y + 5]
        if not chars_below: return None
        chars_below.sort(key=lambda c: c["top"])
        lines = []
        current_line, current_y = [], None
        for char in chars_below:
            if current_y is None or abs(char["top"] - current_y) <= 3:
                current_line.append(char)
                current_y = char["top"]
            else:
                if current_line: lines.append(current_line)
                current_line, current_y = [char], char["top"]
        if current_line: lines.append(current_line)
        for line_chars in lines:
            line_chars.sort(key=lambda c: c["x0"])
            line_text = "".join(c["text"] for c in line_chars).strip()
            for keyword in target_keywords:
                if keyword in line_text:
                    line_x_center = sum(c["x0"] for c in line_chars) / len(line_chars)
                    if abs(line_x_center - keyword_x_center) <= x_tolerance: return keyword
    except Exception:
        pass
    return None

def process_laminate_result(laminate_string):
    """X·ª≠ l√Ω laminate result v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        target_keywords = ["FLEX PAPER/PAPER", "GLUEABLE LAM", "LAM", "RAW", "GRAIN"]
        if not laminate_string or laminate_string.strip() == "": return ""
        parts = [part.strip() for part in laminate_string.split(" / ")]
        if not parts: return ""
        clusters = [part for part in parts if "/" in part]
        if not clusters:
            for keyword in target_keywords:
                if keyword in parts: return keyword
            return parts[-1] if parts else ""
        best_cluster, best_priority = "", float('inf')
        for cluster in clusters:
            cluster_keywords = cluster.split("/")
            cluster_priority = float('inf')
            for keyword in cluster_keywords:
                keyword = keyword.strip()
                if keyword in target_keywords:
                    priority = target_keywords.index(keyword)
                    if priority < cluster_priority: cluster_priority = priority
            if cluster_priority < best_priority: best_priority, best_cluster = cluster_priority, cluster
        if not best_cluster: best_cluster = clusters[-1]
        return best_cluster
    except Exception:
        return ""

def find_profile_a_safe(pdf_path):
    """T√¨m profile v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        with safe_pdf_processing(pdf_path, timeout=15) as pdf:
            for page in pdf.pages:
                try:
                    text = page.extract_text()
                    if text:
                        match = re.search(r"PROFILE\s*:*\s*(\S+)", text, re.IGNORECASE)
                        if match: return match.group(1)
                except Exception:
                    continue
    except Exception:
        pass
    return ""

def extract_edgeband_and_foil_keywords_safe(pdf_path):
    """Tr√≠ch xu·∫•t edgeband v√† foil v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        edgeband_L_keywords = {"EDGEBAND"}
        edgeband_S_keywords = {"DNABEGDE"}
        foil_L_keywords = {"FOIL"}
        foil_S_keywords = {"LIOF"}

        edgeband_L_count, edgeband_S_count = 0, 0
        foil_L_count, foil_S_count = 0, 0

        with safe_pdf_processing(pdf_path, timeout=20) as pdf:
            for page in pdf.pages:
                try:
                    words = page.extract_words(x_tolerance=2)
                    if not words: continue
                    
                    page_words = [w["text"].upper() for w in words]

                    edgeband_L_count += sum(1 for t in page_words if t in edgeband_L_keywords)
                    edgeband_S_count += sum(1 for t in page_words if t in edgeband_S_keywords)
                    foil_L_count += sum(1 for t in page_words if t in foil_L_keywords)
                    foil_S_count += sum(1 for t in page_words if t in foil_S_keywords)
                except Exception:
                    continue
                
        # √Åp d·ª•ng gi·ªõi h·∫°n
        edgeband_L_count, edgeband_S_count = min(edgeband_L_count, 2), min(edgeband_S_count, 2)
        foil_L_count, foil_S_count = min(foil_L_count, 2), min(foil_S_count, 2)

        # T·∫°o chu·ªói k·∫øt qu·∫£ cho Edgeband
        edgeband_result = ""
        if edgeband_L_count > 0: edgeband_result += f"{edgeband_L_count}L"
        if edgeband_S_count > 0: edgeband_result += f"{edgeband_S_count}S"

        # T·∫°o chu·ªói k·∫øt qu·∫£ cho Foil
        foil_result = ""
        if foil_L_count > 0: foil_result += f"{foil_L_count}L"
        if foil_S_count > 0: foil_result += f"{foil_S_count}S"

        return {'Edgeband': edgeband_result, 'Foil': foil_result}
    except Exception:
        return {'Edgeband': '', 'Foil': ''}

def check_dimensions_status(length, width, height):
    """Ki·ªÉm tra status dimensions"""
    try:
        if (length and str(length) != '' and str(length) != 'ERROR' and 
            width and str(width) != '' and str(width) != 'ERROR' and 
            height and str(height) != '' and str(height) != 'ERROR'):
            return 'Done'
    except Exception:
        pass
    return 'Recheck'

def process_single_pdf_safe(pdf_path, original_filename, timeout=120):
    """X·ª≠ l√Ω m·ªôt file PDF v·ªõi timeout v√† x·ª≠ l√Ω l·ªói to√†n di·ªán"""
    start_time = time.time()
    
    try:
        # Ki·ªÉm tra file t·ªìn t·∫°i
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {pdf_path}")
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
        file_size = os.path.getsize(pdf_path)
        if file_size > 50 * 1024 * 1024:  # 50MB
            st.warning(f"‚ö†Ô∏è File {original_filename} l·ªõn ({file_size//1024//1024}MB), c√≥ th·ªÉ x·ª≠ l√Ω ch·∫≠m")
        
        # Tr√≠ch xu·∫•t numbers
        numbers = extract_all_numbers_safe(pdf_path)
        
        # Ki·ªÉm tra timeout
        if time.time() - start_time > timeout:
            raise TimeoutError(f"X·ª≠ l√Ω file qu√° l√¢u (>{timeout}s)")
        
        dim_map = {}
        if numbers:
            try:
                full_df = pd.DataFrame(numbers)
                full_df = assign_ink_groups(full_df, tolerance=1.0)
                full_df['Ink Area Group Count'] = full_df.groupby('Ink Area Group')['Ink Area'].transform('count')
                
                qualified_groups_df = full_df[full_df['Ink Area Group Count'] >= 3].copy()
                
                if not qualified_groups_df.empty:
                    qualified_groups_df['Max Ink Area'] = qualified_groups_df.groupby('Ink Area Group')['Ink Area'].transform('max')
                    sorted_qualified_groups = qualified_groups_df.sort_values(by='Max Ink Area', ascending=False)
                    top_ink_area_group_id = sorted_qualified_groups.iloc[0]['Ink Area Group']
                    top_group_df = qualified_groups_df[qualified_groups_df['Ink Area Group'] == top_ink_area_group_id]
                    unique_numbers_in_group = sorted(top_group_df['Number'].unique())
                    
                    if len(unique_numbers_in_group) >= 1: dim_map[unique_numbers_in_group[-1]] = 'Length (mm)'
                    if len(unique_numbers_in_group) >= 2: dim_map[unique_numbers_in_group[0]] = 'Height (mm)'
                    if len(unique_numbers_in_group) >= 3: dim_map[unique_numbers_in_group[1]] = 'Width (mm)'
                else:
                    high_confidence_dims = full_df[full_df['Confidence (%)'] > 50]
                    if not high_confidence_dims.empty:
                        top_dims = high_confidence_dims.drop_duplicates(subset=['Number']).head(3)
                        sorted_dims = top_dims.sort_values(by='Number', ascending=False)['Number'].tolist()
                        if len(sorted_dims) >= 1: dim_map[sorted_dims[0]] = 'Length (mm)'
                        if len(sorted_dims) >= 3: dim_map[sorted_dims[1]] = 'Width (mm)'; dim_map[sorted_dims[2]] = 'Height (mm)'
                        elif len(sorted_dims) == 2: dim_map[sorted_dims[1]] = 'Width (mm)'
            except Exception as e:
                st.warning(f"L·ªói x·ª≠ l√Ω dimensions cho {original_filename}: {str(e)}")

        # Tr√≠ch xu·∫•t c√°c th√¥ng tin kh√°c
        try:
            laminate_pairs = find_laminate_keywords_safe(pdf_path)
            laminate_raw_result = " / ".join(laminate_pairs) if laminate_pairs else ""
            laminate_result = process_laminate_result(laminate_raw_result) if laminate_pairs else ""
        except Exception:
            laminate_result = ""
            
        try:
            profile_a_result = find_profile_a_safe(pdf_path)
        except Exception:
            profile_a_result = ""
            
        try:
            edgeband_foil_results = extract_edgeband_and_foil_keywords_safe(pdf_path)
        except Exception:
            edgeband_foil_results = {'Edgeband': '', 'Foil': ''}

        final_result = {
            'Drawing #': os.path.splitext(original_filename)[0],
            'Length (mm)': next((k for k, v in dim_map.items() if v == 'Length (mm)'), ''),
            'Width (mm)': next((k for k, v in dim_map.items() if v == 'Width (mm)'), ''),
            'Height (mm)': next((k for k, v in dim_map.items() if v == 'Height (mm)'), ''),
            'Laminate': laminate_result,
            'Edgeband': edgeband_foil_results['Edgeband'],
            'Foil': edgeband_foil_results['Foil'],
            'Profile': profile_a_result
        }
        
        final_result['Status'] = check_dimensions_status(
            final_result['Length (mm)'], 
            final_result['Width (mm)'], 
            final_result['Height (mm)']
        )
        
        # Log th·ªùi gian x·ª≠ l√Ω
        processing_time = time.time() - start_time
        if processing_time > 30:
            st.info(f"‚è±Ô∏è File {original_filename} x·ª≠ l√Ω trong {processing_time:.1f}s")
        
        return final_result
        
    except TimeoutError as e:
        st.error(f"‚è∞ Timeout: {str(e)}")
        return create_error_result(original_filename, "TIMEOUT")
    except Exception as e:
        st.error(f"‚ùå L·ªói x·ª≠ l√Ω {original_filename}: {str(e)}")
        # Log chi ti·∫øt l·ªói ƒë·ªÉ debug
        error_details = traceback.format_exc()
        st.expander("Chi ti·∫øt l·ªói").code(error_details)
        return create_error_result(original_filename, "ERROR")

def create_error_result(filename, error_type="ERROR"):
    """T·∫°o k·∫øt qu·∫£ l·ªói chu·∫©n"""
    return {
        'Drawing #': os.path.splitext(filename)[0],
        'Length (mm)': error_type, 'Width (mm)': error_type, 'Height (mm)': error_type,
        'Laminate': error_type, 'Edgeband': error_type, 'Foil': error_type,
        'Profile': error_type, 'Status': error_type
    }

def to_excel(df):
    """Export to Excel v·ªõi x·ª≠ l√Ω l·ªói"""
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer: 
            df.to_excel(writer, index=False, sheet_name='PDF_Extraction_Results')
    except ImportError:
        try:
            with pd.ExcelWriter(output, engine='openpyxl') as writer: 
                df.to_excel(writer, index=False, sheet_name='PDF_Extraction_Results')
        except ImportError: 
            return None
    except Exception:
        return None
    return output.getvalue()

# ===== GIAO DI·ªÜN STREAMLIT AN TO√ÄN =====
def main():
    st.title("üìÑ Tr√¨nh tr√≠ch xu·∫•t d·ªØ li·ªáu PDF (Safe Mode)")
    st.write("T·ª± ƒë·ªông nh·∫≠n di·ªán k√≠ch th∆∞·ªõc v√† th√¥ng tin t·ª´ b·∫£n v·∫Ω k·ªπ thu·∫≠t v·ªõi x·ª≠ l√Ω l·ªói to√†n di·ªán.")

    uploaded_files = st.file_uploader(
        "K√©o v√† th·∫£ file PDF v√†o ƒë√¢y ho·∫∑c nh·∫•n ƒë·ªÉ ch·ªçn",
        type="pdf",
        accept_multiple_files=True,
        label_visibility="collapsed"
    )
    
    if uploaded_files:
        all_final_results = []
        total_files = len(uploaded_files)
        
        # Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan
        st.info(f"üìä S·∫Ω x·ª≠ l√Ω {total_files} file PDF")
        
        # T·∫°o container cho progress v√† status
        progress_container = st.container()
        
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()
            file_status = st.empty()
            time_info = st.empty()
        
        start_total_time = time.time()
        
        # X·ª≠ l√Ω t·ª´ng file
        for i, uploaded_file in enumerate(uploaded_files):
            current_file = i + 1
            file_start_time = time.time()
            
            # C·∫≠p nh·∫≠t tr·∫°ng th√°i
            status_text.text(f"‚è≥ ƒêang x·ª≠ l√Ω file {current_file}/{total_files}")
            file_status.info(f"üìÑ **{uploaded_file.name}** (K√≠ch th∆∞·ªõc: {uploaded_file.size//1024}KB)")
            
            temp_path = None
            try:
                # L∆∞u file t·∫°m th·ªùi
                temp_path = save_uploaded_file_safe(uploaded_file)
                if not temp_path:
                    raise Exception("Kh√¥ng th·ªÉ l∆∞u file t·∫°m th·ªùi")
                
                # X·ª≠ l√Ω file
                final_result = process_single_pdf_safe(temp_path, uploaded_file.name, timeout=120)
                all_final_results.append(final_result)
                
                # T√≠nh th·ªùi gian x·ª≠ l√Ω
                file_time = time.time() - file_start_time
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                if final_result['Status'] == 'Done':
                    file_status.success(f"‚úÖ **{uploaded_file.name}** - Th√†nh c√¥ng ({file_time:.1f}s)")
                elif final_result['Status'] == 'Recheck':
                    file_status.warning(f"‚ö†Ô∏è **{uploaded_file.name}** - C·∫ßn ki·ªÉm tra l·∫°i ({file_time:.1f}s)")
                else:
                    file_status.error(f"‚ùå **{uploaded_file.name}** - L·ªói ({file_time:.1f}s)")
                    
            except Exception as e:
                error_result = create_error_result(uploaded_file.name, "CRITICAL_ERROR")
                all_final_results.append(error_result)
                file_status.error(f"üí• **{uploaded_file.name}** - L·ªói nghi√™m tr·ªçng: {str(e)}")
                
            finally:
                # D·ªçn d·∫πp file t·∫°m th·ªùi
                if temp_path:
                    cleanup_success = cleanup_temp_file(temp_path)
                    if not cleanup_success:
                        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ d·ªçn d·∫πp file t·∫°m cho {uploaded_file.name}")
                
                # Force garbage collection
                gc.collect()
            
            # C·∫≠p nh·∫≠t progress bar
            progress_percentage = current_file / total_files
            progress_bar.progress(progress_percentage)
            
            # Hi·ªÉn th·ªã th·ªùi gian ∆∞·ªõc t√≠nh
            elapsed_time = time.time() - start_total_time
            if current_file > 1:
                avg_time_per_file = elapsed_time / current_file
                remaining_files = total_files - current_file
                eta = remaining_files * avg_time_per_file
                time_info.text(f"‚è±Ô∏è ƒê√£ qua: {elapsed_time:.0f}s | ∆Ø·ªõc t√≠nh c√≤n l·∫°i: {eta:.0f}s")
        
        # Ho√†n th√†nh x·ª≠ l√Ω
        total_time = time.time() - start_total_time
        progress_bar.progress(1.0)
        status_text.success(f"‚úÖ Ho√†n th√†nh x·ª≠ l√Ω {total_files} file trong {total_time:.1f}s!")
        file_status.empty()
        time_info.empty()
        
        if all_final_results:
            st.markdown("---")
            
            # Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng quan
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                done_count = sum(1 for result in all_final_results if result['Status'] == 'Done')
                st.metric("‚úÖ Th√†nh c√¥ng", done_count)
            with col2:
                recheck_count = sum(1 for result in all_final_results if result['Status'] == 'Recheck')
                st.metric("‚ö†Ô∏è C·∫ßn ki·ªÉm tra", recheck_count)
            with col3:
                error_count = sum(1 for result in all_final_results if 'ERROR' in str(result['Status']))
                st.metric("‚ùå L·ªói", error_count)
            with col4:
                success_rate = (done_count / total_files * 100) if total_files > 0 else 0
                st.metric("üìà T·ª∑ l·ªá th√†nh c√¥ng", f"{success_rate:.1f}%")
            
            st.markdown("---")
            st.subheader("üìä K·∫øt qu·∫£ tr√≠ch xu·∫•t chi ti·∫øt")
            
            final_results_df = pd.DataFrame(all_final_results)
            
            # T√¥ m√†u theo status
            def highlight_status(val):
                if val == 'Done':
                    return 'background-color: #d4edda; color: #155724'
                elif val == 'Recheck':
                    return 'background-color: #fff3cd; color: #856404'
                elif 'ERROR' in str(val) or 'TIMEOUT' in str(val):
                    return 'background-color: #f8d7da; color: #721c24'
                return ''
            
            styled_df = final_results_df.style.applymap(highlight_status, subset=['Status'])
            st.dataframe(styled_df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
            
            # N√∫t download
            col1, col2 = st.columns(2)
            with col1:
                excel_data = to_excel(final_results_df)
                if excel_data:
                    st.download_button(
                        label="üì• T·∫£i v·ªÅ file Excel",
                        data=excel_data,
                        file_name=f"pdf_extraction_results_{total_files}_files_{int(time.time())}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                else:
                    st.button("üìä Excel (Kh√¥ng kh·∫£ d·ª•ng)", disabled=True, 
                             help="C·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán xlsxwriter ho·∫∑c openpyxl")
            
            with col2:
                csv_data = final_results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üìÑ T·∫£i v·ªÅ file CSV", 
                    data=csv_data,
                    file_name=f"pdf_extraction_results_{total_files}_files_{int(time.time())}.csv",
                    mime="text/csv"
                )
    
    else:
        st.info("üëÜ Vui l√≤ng t·∫£i l√™n m·ªôt ho·∫∑c nhi·ªÅu file PDF ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        
        # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        with st.expander("üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"):
            st.markdown("""
            ### Phi√™n b·∫£n Safe Mode - T√≠nh nƒÉng m·ªõi:
            - ‚úÖ **Timeout Protection**: T·ª± ƒë·ªông ng·∫Øt file x·ª≠ l√Ω qu√° l√¢u
            - ‚úÖ **Memory Management**: D·ªçn d·∫πp b·ªô nh·ªõ sau m·ªói file
            - ‚úÖ **Error Recovery**: Ti·∫øp t·ª•c x·ª≠ l√Ω c√°c file kh√°c khi g·∫∑p l·ªói
            - ‚úÖ **File Size Warning**: C·∫£nh b√°o file qu√° l·ªõn
            - ‚úÖ **Progress Tracking**: Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω chi ti·∫øt
            - ‚úÖ **Safe File Handling**: Qu·∫£n l√Ω file t·∫°m th·ªùi an to√†n
            
            ### C√°ch s·ª≠ d·ª•ng:
            1. **T·∫£i file PDF**: Click ho·∫∑c k√©o th·∫£ file PDF
            2. **Theo d√µi ti·∫øn tr√¨nh**: Xem progress bar v√† th·ªùi gian ∆∞·ªõc t√≠nh
            3. **Xem k·∫øt qu·∫£ realtime**: K·∫øt qu·∫£ hi·ªÉn th·ªã ngay khi x·ª≠ l√Ω xong t·ª´ng file
            4. **Download k·∫øt qu·∫£**: Excel ho·∫∑c CSV v·ªõi timestamp
            
            ### Gi·ªõi h·∫°n v√† khuy·∫øn ngh·ªã:
            - File t·ªëi ƒëa: 50MB/file
            - Timeout: 120s/file
            - Khuy·∫øn ngh·ªã: < 20 file/l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o performance
            """)
    
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; font-size: 0.9em;'>
        PDF Data Extractor v3.0 Safe Mode | Enhanced Error Handling & Performance
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
